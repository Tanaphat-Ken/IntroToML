{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla RNN from Scratch (NumPy)\n",
        "\n",
        "This cell implements a **minimal RNN** from scratch using only NumPy.  \n",
        "It is broken down into several classes:\n",
        "\n",
        "- **InputLayer**: handles one-hot encoded inputs and multiplies with input weights `U`  \n",
        "- **HiddenLayer**: maintains hidden states (`a_t`) and applies `tanh` activation  \n",
        "- **OutputLayer**: computes logits and applies `softmax` for next-character prediction  \n",
        "- **VanillaRNN**: ties everything together, implementing:\n",
        "  - forward pass  \n",
        "  - BPTT (Backpropagation Through Time)  \n",
        "  - parameter update (`step`)  \n",
        "\n",
        "Additionally, helper functions are defined:\n",
        "- `softmax`: normalization for logits  \n",
        "- `one_hot`: converts character index into one-hot vector  \n",
        "- `make_sequence_data`: generates synthetic training data (alphabet sequences)\n",
        "\n",
        "**Goal:** Train an RNN to predict the next character in the A–Z alphabet sequence.\n",
        "\n",
        "Source : https://medium.com/@thisislong/building-a-recurrent-neural-network-from-scratch-ba9b27a42856"
      ],
      "metadata": {
        "id": "IwIyPHbNc5_6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QRHzdCoJ_b0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "def softmax(z: np.ndarray) -> np.ndarray:\n",
        "    z = z - np.max(z, axis=0, keepdims=True)\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "def one_hot(idx: int, vocab_size: int) -> np.ndarray:\n",
        "    v = np.zeros((vocab_size, 1), dtype=np.float32)\n",
        "    v[idx, 0] = 1.0\n",
        "    return v\n",
        "\n",
        "class InputLayer:\n",
        "    def __init__(self, input_size: int, hidden_size: int, seed: int = 42):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.U = rng.normal(0, 0.1, size=(hidden_size, input_size)).astype(np.float32)\n",
        "        self.b = np.zeros((hidden_size, 1), dtype=np.float32)\n",
        "        self.dU = np.zeros_like(self.U)\n",
        "        self.db = np.zeros_like(self.b)\n",
        "        self.inputs: List[np.ndarray] = []\n",
        "\n",
        "    def get_input(self, t: int) -> np.ndarray:\n",
        "        return self.inputs[t]\n",
        "\n",
        "    def weighted_sum(self, t: int) -> np.ndarray:\n",
        "        return self.U @ self.get_input(t) + self.b\n",
        "\n",
        "    def zero_cache(self):\n",
        "        self.inputs = []\n",
        "\n",
        "    def zero_grads(self):\n",
        "        self.dU.fill(0.0)\n",
        "        self.db.fill(0.0)\n",
        "\n",
        "    def accumulate_grads(self, delta_s_t: np.ndarray, t: int):\n",
        "        self.dU += delta_s_t @ self.get_input(t).T\n",
        "        self.db += delta_s_t\n",
        "\n",
        "    def step(self, lr: float, clip: float = None):\n",
        "        if clip is not None:\n",
        "            np.clip(self.dU, -clip, clip, out=self.dU)\n",
        "            np.clip(self.db, -clip, clip, out=self.db)\n",
        "        self.U -= lr * self.dU\n",
        "        self.b -= lr * self.db\n",
        "\n",
        "class HiddenLayer:\n",
        "    def __init__(self, hidden_size: int, seed: int = 123):\n",
        "        self.hidden_size = hidden_size\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.W = rng.normal(0, 0.1, size=(hidden_size, hidden_size)).astype(np.float32)\n",
        "        self.b = np.zeros((hidden_size, 1), dtype=np.float32)\n",
        "        self.dW = np.zeros_like(self.W)\n",
        "        self.db = np.zeros_like(self.b)\n",
        "        self.a: List[np.ndarray] = []\n",
        "        self.s: List[np.ndarray] = []\n",
        "\n",
        "    def get_a(self, t: int) -> np.ndarray:\n",
        "        if t < 0:\n",
        "            return np.zeros((self.hidden_size, 1), dtype=np.float32)\n",
        "        return self.a[t]\n",
        "\n",
        "    def zero_cache(self):\n",
        "        self.a = []\n",
        "        self.s = []\n",
        "\n",
        "    def zero_grads(self):\n",
        "        self.dW.fill(0.0)\n",
        "        self.db.fill(0.0)\n",
        "\n",
        "    def forward_step(self, s_t: np.ndarray) -> np.ndarray:\n",
        "        a_t = np.tanh(s_t)\n",
        "        self.s.append(s_t)\n",
        "        self.a.append(a_t)\n",
        "        return a_t\n",
        "\n",
        "    def accumulate_grads(self, delta_s_t: np.ndarray, t: int):\n",
        "        a_prev = self.get_a(t - 1)\n",
        "        self.dW += delta_s_t @ a_prev.T\n",
        "        self.db += delta_s_t\n",
        "\n",
        "    def step(self, lr: float, clip: float = None):\n",
        "        if clip is not None:\n",
        "            np.clip(self.dW, -clip, clip, out=self.dW)\n",
        "            np.clip(self.db, -clip, clip, out=self.db)\n",
        "        self.W -= lr * self.dW\n",
        "        self.b -= lr * self.db\n",
        "\n",
        "class OutputLayer:\n",
        "    def __init__(self, hidden_size: int, output_size: int, seed: int = 7):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.V = rng.normal(0, 0.1, size=(output_size, hidden_size)).astype(np.float32)\n",
        "        self.c = np.zeros((output_size, 1), dtype=np.float32)\n",
        "        self.dV = np.zeros_like(self.V)\n",
        "        self.dc = np.zeros_like(self.c)\n",
        "        self.o: List[np.ndarray] = []\n",
        "        self.yhat: List[np.ndarray] = []\n",
        "\n",
        "    def zero_cache(self):\n",
        "        self.o = []\n",
        "        self.yhat = []\n",
        "\n",
        "    def zero_grads(self):\n",
        "        self.dV.fill(0.0)\n",
        "        self.dc.fill(0.0)\n",
        "\n",
        "    def forward_step(self, a_t: np.ndarray):\n",
        "        o_t = self.V @ a_t + self.c\n",
        "        yhat_t = softmax(o_t)\n",
        "        self.o.append(o_t)\n",
        "        self.yhat.append(yhat_t)\n",
        "        return o_t, yhat_t\n",
        "\n",
        "    def accumulate_grads(self, delta_o_t: np.ndarray, a_t: np.ndarray):\n",
        "        self.dV += delta_o_t @ a_t.T\n",
        "        self.dc += delta_o_t\n",
        "\n",
        "    def step(self, lr: float, clip: float = None):\n",
        "        if clip is not None:\n",
        "            np.clip(self.dV, -clip, clip, out=self.dV)\n",
        "            np.clip(self.dc, -clip, clip, out=self.dc)\n",
        "        self.V -= lr * self.dV\n",
        "        self.c -= lr * self.dc\n",
        "\n",
        "class VanillaRNN:\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int, lr: float = 0.1, clip: float = 5.0):\n",
        "        self.input = InputLayer(input_size, hidden_size)\n",
        "        self.hidden = HiddenLayer(hidden_size)\n",
        "        self.output = OutputLayer(hidden_size, output_size)\n",
        "        self.lr = lr\n",
        "        self.clip = clip\n",
        "\n",
        "    def zero_caches_and_grads(self):\n",
        "        self.input.zero_cache()\n",
        "        self.hidden.zero_cache()\n",
        "        self.output.zero_cache()\n",
        "        self.input.zero_grads()\n",
        "        self.hidden.zero_grads()\n",
        "        self.output.zero_grads()\n",
        "\n",
        "    def forward(self, x_seq: List[np.ndarray]):\n",
        "        self.zero_caches_and_grads()\n",
        "        yhat_seq = []\n",
        "        for t, x_t in enumerate(x_seq):\n",
        "            self.input.inputs.append(x_t)\n",
        "            s_t = self.hidden.W @ self.hidden.get_a(t - 1) + self.input.weighted_sum(t) + self.hidden.b\n",
        "            a_t = self.hidden.forward_step(s_t)\n",
        "            _, yhat_t = self.output.forward_step(a_t)\n",
        "            yhat_seq.append(yhat_t)\n",
        "        return yhat_seq\n",
        "\n",
        "    def bptt(self, y_seq: List[np.ndarray]):\n",
        "        T = len(y_seq)\n",
        "        delta_a_next = np.zeros((self.hidden.hidden_size, 1), dtype=np.float32)\n",
        "        for t in reversed(range(T)):\n",
        "            a_t = self.hidden.a[t]\n",
        "            yhat_t = self.output.yhat[t]\n",
        "            y_true_t = y_seq[t]\n",
        "            delta_o_t = (yhat_t - y_true_t)\n",
        "            self.output.accumulate_grads(delta_o_t, a_t)\n",
        "            delta_a_t = self.output.V.T @ delta_o_t + delta_a_next\n",
        "            ds_t = (1.0 - np.square(a_t)) * delta_a_t\n",
        "            self.input.accumulate_grads(ds_t, t)\n",
        "            self.hidden.accumulate_grads(ds_t, t)\n",
        "            delta_a_next = self.hidden.W.T @ ds_t\n",
        "\n",
        "    def step(self):\n",
        "        self.input.step(self.lr, self.clip)\n",
        "        self.hidden.step(self.lr, self.clip)\n",
        "        self.output.step(self.lr, self.clip)\n",
        "\n",
        "    def loss(self, yhat_seq: List[np.ndarray], y_seq: List[np.ndarray]) -> float:\n",
        "        return sum(-float(np.sum(y_seq[t]*np.log(yhat_seq[t]+1e-12))) for t in range(len(y_seq)))\n",
        "\n",
        "    def predict_next(self, x_seq: List[np.ndarray]) -> int:\n",
        "        yhat_seq = self.forward(x_seq)\n",
        "        last = yhat_seq[-1]\n",
        "        return int(np.argmax(last))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions\n",
        "\n",
        "This cell defines utility functions:\n",
        "\n",
        "- `one_hot`: returns a one-hot encoded column vector for a given index.  \n",
        "- `make_sequence_data`: generates synthetic sequential data:\n",
        "  - Randomly selects a starting index `k` in the alphabet.  \n",
        "  - Creates an input sequence of length `seq_len`.  \n",
        "  - The target sequence is shifted by one character.  \n",
        "\n",
        "For example, input: `[\"A\",\"B\",\"C\",\"D\"]` → target: `[\"B\",\"C\",\"D\",\"E\"]`.\n"
      ],
      "metadata": {
        "id": "Ev28YAvwdJco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(idx: int, vocab_size: int) -> np.ndarray:\n",
        "    v = np.zeros((vocab_size, 1), dtype=np.float32)\n",
        "    v[idx, 0] = 1.0\n",
        "    return v\n",
        "\n",
        "def make_sequence_data(seq_len: int = 5, num_samples: int = 200):\n",
        "    rng = np.random.default_rng(0)\n",
        "    dataset = []\n",
        "    for _ in range(num_samples):\n",
        "        k = int(rng.integers(0, V))\n",
        "        letters = [vocab[(k + t) % V] for t in range(seq_len + 1)]\n",
        "        xs = [one_hot(v2i[ch], V) for ch in letters[:-1]]\n",
        "        ys = [one_hot(v2i[ch], V) for ch in letters[1:]]\n",
        "        dataset.append((xs, ys))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Tiru2mhrKGkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Dataset\n",
        "\n",
        "This cell sets up the dataset:\n",
        "\n",
        "- Defines the alphabet `A–Z` and vocabulary mappings:  \n",
        "  - `v2i`: character → index  \n",
        "  - `i2v`: index → character  \n",
        "- Defines vocabulary size `V`.  \n",
        "- Generates the training dataset using `make_sequence_data` with 300 samples.  \n",
        "\n",
        "Example of one training sample:\n",
        "- Input (x): `[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]`  \n",
        "- Target (y): `[\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"]`  \n",
        "\n",
        "Each character is represented as a one-hot vector of size `V=26`.\n"
      ],
      "metadata": {
        "id": "Mt0ksV3hdNhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "vocab = list(alphabet)\n",
        "v2i = {ch: i for i, ch in enumerate(vocab)}\n",
        "i2v = {i: ch for ch, i in v2i.items()}\n",
        "V = len(vocab)\n",
        "\n",
        "seq_len = 6\n",
        "train = make_sequence_data(seq_len=seq_len, num_samples=300)\n",
        "\n",
        "## 1 sample (seq_len = 6) : (x 6 feature, y 6 feature)\n",
        "# Ex : x (ABCDEF) -> y (BCDEFG)\n",
        "\n",
        "## 1 feature 24 size (one-hot encoding)\n",
        "# ex : A -> [1, 0, 0, 0 ..., 0]\n",
        "# ex : B -> [0, 1, 0, 0 ..., 0]\n",
        "# ex : Z -> [0, 0, 0, 0 ..., 1]\n",
        "print(train[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifPppRMYKN2k",
        "outputId": "1e77261e-b6a6-4861-8f27-e1e896017075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32), array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32), array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.]], dtype=float32), array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.]], dtype=float32), array([[1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32), array([[0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32)], [array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32), array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.]], dtype=float32), array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.]], dtype=float32), array([[1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32), array([[0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32), array([[0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing the RNN\n",
        "\n",
        "This cell trains the RNN model:\n",
        "\n",
        "- Initializes a `VanillaRNN` with:\n",
        "  - `input_size = V` (26)  \n",
        "  - `hidden_size = 64`  \n",
        "  - `output_size = V`  \n",
        "- Trains for **15 epochs** using cross-entropy loss.  \n",
        "- Performs stochastic gradient descent with gradient clipping (`clip=5.0`).  \n",
        "\n",
        "At the end, it tests the model on the sequence `\"ABCDE\"`,  \n",
        "and predicts the **next character** (expected `\"F\"`).  \n"
      ],
      "metadata": {
        "id": "nlRzw5qNdRgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## input size = output size , hidden size = 64\n",
        "rnn = VanillaRNN(input_size=V, hidden_size=64, output_size=V, lr=0.1, clip=5.0)\n",
        "for epoch in range(1, 101):\n",
        "    np.random.shuffle(train)\n",
        "    total = 0.0\n",
        "    for xs, ys in train:\n",
        "        yhat = rnn.forward(xs)\n",
        "        L = sum(-float(np.sum(ys[t]*np.log(yhat[t]+1e-12))) for t in range(len(ys)))\n",
        "        rnn.bptt(ys)\n",
        "        rnn.step()\n",
        "        total += L\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"epoch={epoch} loss={total/len(train):.4f}\")\n",
        "\n",
        "test_seq = \"ABCDEF\"\n",
        "xs = [one_hot(v2i[ch], V) for ch in test_seq]\n",
        "pred = rnn.predict_next(xs)\n",
        "print(\"Input:\", test_seq, \" Pred next:\", i2v[pred])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4TrdiUUKjCe",
        "outputId": "ebdaad35-a9e4-46fa-aa0a-c06ba13c1d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=10 loss=0.0086\n",
            "epoch=20 loss=0.0037\n",
            "epoch=30 loss=0.0023\n",
            "epoch=40 loss=0.0017\n",
            "epoch=50 loss=0.0013\n",
            "epoch=60 loss=0.0011\n",
            "epoch=70 loss=0.0009\n",
            "epoch=80 loss=0.0008\n",
            "epoch=90 loss=0.0007\n",
            "epoch=100 loss=0.0006\n",
            "Input: ABCDEF  Pred next: G\n"
          ]
        }
      ]
    }
  ]
}