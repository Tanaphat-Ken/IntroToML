{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683895bb",
   "metadata": {},
   "source": [
    "# Cheatsheet - วิธีทำ Regression & Classification (Step-by-step)\n",
    "ไฟล์นี้สรุปวิธีทำสำหรับแต่ละอัลกอริทึมที่ขอ (ไทย) พร้อมตัวอย่างโค้ดสั้นๆ เพื่อให้ใช้ได้ทันทีตอนสอบ\n",
    "แต่ละหัวข้อมี: สรุป, inputs/outputs, ขั้นตอนปฏิบัติ, hyperparameters ที่ควร tune, และตัวอย่างโค้ดสั้นๆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e217c2",
   "metadata": {},
   "source": [
    "## หมายเหตุก่อนเริ่ม (General checklist)\n",
    "- แยกข้อมูล: train/test (และ validation หรือใช้ cross-validation).\n",
    "- ตรวจสอบ missing values และ data types (df.info(), df.isnull().sum()).\n",
    "- Scale features ถ้าจำเป็น (SVM, KNN, Logistic, NN).\n",
    "- ใช้ cross-validation (GridSearchCV) เพื่อ tune hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050ac33",
   "metadata": {},
   "source": [
    "## Regression Methods\n",
    "### 1) Linear Regression (Simple / Multiple)\n",
    "สรุป: แบบพื้นฐาน ใช้เมื่อความสัมพันธ์เป็นเชิงเส้น\n",
    "Inputs: numeric features (1 or more). Output: continuous target.\n",
    "Steps:\n",
    "1. แบ่ง train/test\n",
    "2. (Optional) scale features (not necessary for OLS but ok)\n",
    "3. fit LinearRegression()\n",
    "4. evaluate ด้วย R², RMSE, MAE\n",
    "Hyperparams: ไม่มีมาก (OLS) — ถ้าจำเป็นให้ใช้ regularization (Ridge/Lasso)\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear / Multiple Linear Regression (scikit-learn)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('R2', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c377f49d",
   "metadata": {},
   "source": [
    "### 2) Polynomial Regression\n",
    "สรุป: ขยาย features ด้วย PolynomialFeatures เมื่อความสัมพันธ์เป็น non-linear โดยยังใช้ linear model\n",
    "Steps:\n",
    "1. ใช้ PolynomialFeatures(degree=k, include_bias=False) -> poly.fit_transform(X)\n",
    "2. Fit LinearRegression on transformed features\n",
    "3. Evaluate และระวัง overfitting (เพิ่ม degree -> overfit)\n",
    "Hyperparams: degree (2,3...), interaction_only optional\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abcb86",
   "metadata": {},
   "source": [
    "### 3) Ridge (L2 regularization) & Lasso (L1)\n",
    "สรุป: Regularized linear models ช่วยลด overfitting และ (Lasso) ทำ feature selection ได้บางส่วน\n",
    "Steps:\n",
    "1. Standardize features (important)\n",
    "2. ใช้ Ridge(alpha=...) หรือ Lasso(alpha=...)\n",
    "3. Tune alpha via cross-validation (GridSearchCV / RidgeCV / LassoCV)\n",
    "Hyperparams: alpha (regularization strength)\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sc = StandardScaler()\n",
    "Xs = sc.fit_transform(X)\n",
    "params = {'alpha':[0.01,0.1,1,10]}\n",
    "grid = GridSearchCV(Ridge(), params, cv=5)\n",
    "grid.fit(Xs, y)\n",
    "print('best alpha', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba68150",
   "metadata": {},
   "source": [
    "### 4) When to pick which (Regression) — Quick advice\n",
    "- Small dataset, linear relation: Linear / Ridge (if noise)\n",
    "- Non-linear but low-dim: Polynomial (with care)\n",
    "- Many features / potential multicollinearity: Ridge/Lasso\n",
    "- Complex patterns / high accuracy target: tree-based ensembles (RandomForest, XGBoost) — usually gives best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8b4da",
   "metadata": {},
   "source": [
    "## Classification Methods\n",
    "### 1) Logistic Regression\n",
    "สรุป: baseline binary classifier; outputs probabilities; good baseline and interpretable coefficients\n",
    "Steps:\n",
    "1. Split train/test, scale features\n",
    "2. Fit LogisticRegression(max_iter=1000, C=...) (C is inverse reg strength)\n",
    "3. Evaluate with accuracy, precision, recall, f1\n",
    "Hyperparams: C (regularization), penalty ('l2' default)\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "Xtr = sc.fit_transform(X_train)\n",
    "Xte = sc.transform(X_test)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(Xtr, y_train)\n",
    "print(clf.score(Xte, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6c5be",
   "metadata": {},
   "source": [
    "### 2) Decision Tree\n",
    "สรุป: tree-based model; interpretability (visualize tree), sensitive to depth -> prune or set max_depth\n",
    "Steps:\n",
    "1. Split data (no need to scale)\n",
    "2. Fit DecisionTreeClassifier(max_depth=...)\n",
    "3. Tune max_depth, min_samples_split via CV\n",
    "Hyperparams: max_depth, min_samples_split, criterion ('gini'/'entropy')\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fb726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('acc', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e1767",
   "metadata": {},
   "source": [
    "### 3) Ensemble: Random Forest\n",
    "สรุป: Bagging of decision trees, less overfitting than single tree, good default for tabular data\n",
    "Steps:\n",
    "1. Fit RandomForestClassifier(n_estimators=100)\n",
    "2. Tune n_estimators, max_depth, max_features via CV\n",
    "3. Check feature_importances_ for interpretability\n",
    "Hyperparams: n_estimators, max_depth, max_features\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print('acc', rf.score(X_test, y_test))\n",
    "print('imp', rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d9d13",
   "metadata": {},
   "source": [
    "### 4) Naïve Bayes\n",
    "สรุป: สมมติ independence ระหว่าง features; เหมาะกับ text (Bag-of-Words) หรือ categorical data\n",
    "Steps:\n",
    "1. Vectorize text (CountVectorizer / Tfidf)\n",
    "2. Fit MultinomialNB or GaussianNB depending on data type\n",
    "Hyperparams: alpha (smoothing) for MultinomialNB\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_text = cv.fit_transform(texts)\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_text_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bce2e",
   "metadata": {},
   "source": [
    "### 5) Support Vector Machine (SVM)\n",
    "สรุป: margin-based classifier; good for medium-sized datasets and high-dimensional space (text). สำคัญ: scale features\n",
    "Steps:\n",
    "1. Scale features (StandardScaler)\n",
    "2. For linear separable try LinearSVC or SVC(kernel='linear'), otherwise use kernel='rbf' with C and gamma tuning\n",
    "Hyperparams: C (regularization), kernel, gamma (for rbf)\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "Xtr = sc.fit_transform(X_train)\n",
    "Xte = sc.transform(X_test)\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svc.fit(Xtr, y_train)\n",
    "print('acc', svc.score(Xte, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6eb5b8",
   "metadata": {},
   "source": [
    "### 6) Dimensionality reduction (PCA) for RandomForest / SVM\n",
    "สรุป: PCA ช่วยลดมิติ ทำให้ SVM เร็วขึ้นและบางครั้งช่วย generalize; RandomForest มักไม่ต้องการ PCA แต่ในกรณีมี noise/very high-dim อาจช่วย\n",
    "Steps:\n",
    "1. Standardize data\n",
    "2. Fit PCA(n_components=k) on train only\n",
    "3. Transform train/test and feed to model\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22208c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "Xs = sc.fit_transform(X)\n",
    "pca = PCA(n_components=10)\n",
    "Xp = pca.fit_transform(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b432535",
   "metadata": {},
   "source": [
    "### 7) Unsupervised: KMeans & Agglomerative Clustering\n",
    "สรุป: สำหรับ clustering tasks (no labels). KMeans ดีสำหรับกลุ่มที่กลมและมี variance เท่าๆ กัน; Agglomerative ดีสำหรับ nested clusters และเมื่อต้องการ dendrogram\n",
    "Steps:\n",
    "1. (Optional) scale data\n",
    "2. Choose number of clusters (elbow, silhouette)\n",
    "3. Fit KMeans or AgglomerativeClustering\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e642de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "km = KMeans(n_clusters=3, random_state=42)\n",
    "labels = km.fit_predict(X)\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "labels2 = agg.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff09c2",
   "metadata": {},
   "source": [
    "### 8) Perceptron & Single-Layer Perceptron (SLP)\n",
    "สรุป: Perceptron เป็น linear classifier (เหมือน Logistic แต่ไม่มี probabilistic output); SLP คือ single-layer neural unit (no hidden layers)\n",
    "Steps:\n",
    "1. Scale features\n",
    "2. Fit Perceptron (sklearn.linear_model.Perceptron) or a single Linear layer in PyTorch/Keras\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bcfa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "Xtr = sc.fit_transform(X_train)\n",
    "clf = Perceptron(max_iter=1000, tol=1e-3)\n",
    "clf.fit(Xtr, y_train)\n",
    "print('acc', clf.score(sc.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c9bbb",
   "metadata": {},
   "source": [
    "### 9) Multi-Layer Perceptron (MLP)\n",
    "สรุป: Feedforward neural network; good when data is large and non-linear patterns exist\n",
    "Steps:\n",
    "1. Scale features\n",
    "2. Choose architecture (hidden layers, units), activation (ReLU), optimizer (Adam)\n",
    "3. Tune learning rate, epochs, regularization\n",
    "Code snippet (sklearn MLPClassifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15919e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "sc = StandardScaler()\n",
    "Xtr = sc.fit_transform(X_train)\n",
    "Xte = sc.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64,32), max_iter=200, random_state=42)\n",
    "mlp.fit(Xtr, y_train)\n",
    "print('acc', mlp.score(Xte, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd8b59",
   "metadata": {},
   "source": [
    "### 10) XGBoost (eXtreme Gradient Boosting)\n",
    "สรุป: Gradient boosting tree; มักให้ผลลัพธ์ดีที่สุดในงาน tabular; ต้องติดตั้ง xgboost หรือ use sklearn wrapper\n",
    "Steps:\n",
    "1. Install xgboost (pip install xgboost)\n",
    "2. Tune n_estimators, learning_rate, max_depth, subsample, colsample_bytree\n",
    "Code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee593270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost example (if installed)\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "print('acc', xgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5350f",
   "metadata": {},
   "source": [
    "## Summary: Which is best (quick picks for exams)\n",
    "- Regression (highest accuracy on tabular): Gradient Boosting (XGBoost / LightGBM) or RandomForest if XGBoost not available.\n",
    "- For small/simple problems: Ridge (stable) or Linear Regression.\n",
    "- Classification (highest accuracy on tabular): XGBoost / LightGBM or RandomForest.\n",
    "- For text: Multinomial Naive Bayes or fine-tuned Logistic/SVM on TF-IDF.\n",
    "- For high-dim small-sample: SVM with kernel (or regularized linear models).\n",
    "Practical exam advice:\n",
    "1) If allowed any model and accuracy matters -> try XGBoost/LightGBM with basic hyperparameter tuning.\n",
    "2) If time-limited-> RandomForest with 100-200 trees + max_depth tuned.\n",
    "3) Always run cross-validation (5-fold) and report the metric requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd8ef0",
   "metadata": {},
   "source": [
    "## Quick cheat-sheet (Commands to run in PowerShell)\n",
    "# Install common packages if missing:\n",
    "pip install scikit-learn xgboost lightgbm shap statsmodels"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
